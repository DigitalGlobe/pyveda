{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing, Working with Veda Datasets via VedaCollection + VedaBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10/30/2018\n",
    "RJP\n",
    "\n",
    "Pyveda is a python library for interacting with machine-learning datasets on the veda cloud. This notebook is meant to serve as a brief tutorial to the various modules and their apis in pyveda, and how they interact. This notebook should be updated as the [development](https://github.com/DigitalGlobe/pyveda/tree/development) branch of pyveda is updated with new functionality.\n",
    "\n",
    "Currently, there are two main veda-data interfaces in pyveda: `VedaCollection` and `VedaBase`. `VedaCollection` is more of a client-side representation of a Veda Dataset, accompanied with an api that supports all current client-server communication (REST), so that editing, deleting, adding new datapoints as well as Datasets are handled on the class instance. `VedaBase` is a high-level wrapper around HDF5 (via pytables) that supports fast read/write of contiguous label, image pairs, custom ML-optimized iteration patterns, custom ML-based filetree structure with fine grain access to each data node and leaf, as well as a query-based api around datapoint metadata (in development), builtin input hooks for plugging into whatever DL framework (in development), and maybe some other stuff, i dunno, some optimization/compression io stuff would be good prob.  \n",
    "\n",
    "The best way to accompany youself with these api's and how they work, besides this notebook, is looking at the code. Here's where [VedaCollection lives](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/training.py), and here's where [VedaBase lives](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/db/vedabase.py). \n",
    "\n",
    "Since the codebase is in development, new dependencies may well be added at any time, so it might be the case that you need to update your env/install whatever is missing if you're working locally.\n",
    "\n",
    "We're going to point to veda-dev for this, so you will need gbdx access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyveda.training.VedaCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SANDMAN_API\"] = \"https://veda-api-development.geobigdata.io\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, if you're trying to point to the non-official veda uri (\"https://veda-api.geobigdata.io\"), and your `SANDMAN_API` env var isn't already set to your target (development in this case see), you need to set that variable before import anything from pyveda, since HOST is set globally there on import. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll import VedaCollection and instantiate an instance via the `from_id` classmethod. The argument is a Veda Dataset Id. Each hosted dataset is associated with a particular guid, and the two I've listed below are currently (at time of writing) available on dev, and (with some minor exceptions) they're up-to-date structure-wise, so we're gonna just stick with those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyveda import VedaCollection\n",
    "\n",
    "seg = \"411656d7-ec2d-4726-a0f2-35d2bd48e165\"\n",
    "objd = \"e18c91fc-d4b7-4cca-97dc-db7d2fb27d6c\"\n",
    "vc = VedaCollection.from_id(objd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other patterns to instantiating a VedaCollection, via `__init__`, as well as who knows how many others? VedaCollection can do a lot of things, but it effectively represents a veda dataset, while providing patterns for generating and posting new, user-created datasets into veda. Here's a quick look at what VedaCollection can tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vc.meta:\n",
      "{'name': 'XView', 'mlType': 'object_detection', 'public': 'true', 'partition': [100, 0, 0], 'image_refs': [], 'classes': ['shipping container lot', 'bulldozer', 'truck tractor no trailer', 'facility', 'personal aircraft', 'shed', 'aircraft hangar', 'shipping container', 'passenger vehicle', 'storage tank', 'construction site', 'truck tractor with flatbed trailer', 'helicopter', 'bus', 'fixed wing aircraft', 'cargo truck', 'passenger aircraft', 'truck tractor with liquid tank', 'container crane', 'damaged building', 'building', 'car', 'tractor trailer', 'utility truck', 'vehicle lot', 'trailer no truck', 'truck', 'dump truck', 'utility vehicle', 'helipad'], 'bounds': [-1.526822244582169, 12.335128140830934, -1.499049247586651, 12.37132678462982], 'user_id': 'auth0|5ac52c560043574440853f22'}\n",
      "\n",
      "vc.count: 1399\n",
      "\n",
      "vc.bounds: [-1.526822244582169, 12.335128140830934, -1.499049247586651, 12.37132678462982]\n",
      "\n",
      "vc.classes:\n",
      "['shipping container lot', 'bulldozer', 'truck tractor no trailer', 'facility', 'personal aircraft', 'shed', 'aircraft hangar', 'shipping container', 'passenger vehicle', 'storage tank', 'construction site', 'truck tractor with flatbed trailer', 'helicopter', 'bus', 'fixed wing aircraft', 'cargo truck', 'passenger aircraft', 'truck tractor with liquid tank', 'container crane', 'damaged building', 'building', 'car', 'tractor trailer', 'utility truck', 'vehicle lot', 'trailer no truck', 'truck', 'dump truck', 'utility vehicle', 'helipad']\n",
      "\n",
      "vc.imshape: (3, 256, 256)\n",
      "\n",
      "vc.mlType: object_detection\n"
     ]
    }
   ],
   "source": [
    "print(\"vc.meta:\\n{}\\n\".format(vc.meta))\n",
    "print(\"vc.count: {}\\n\".format(vc.count))\n",
    "print(\"vc.bounds: {}\\n\".format(vc.bounds))\n",
    "print(\"vc.classes:\\n{}\\n\".format(vc.classes))\n",
    "print(\"vc.imshape: {}\\n\".format(vc.imshape))\n",
    "print(\"vc.mlType: {}\".format(vc.mlType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty useful dataset attributes. They're crucial for knowing how to handle the real, hard data. There's more to be said here about the VedaCollection api specifically, but that's something that somebody will do later probably.\n",
    "TO DO: SAY MORE HERE bout that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VedaCollection -> VedaBase\n",
    "\n",
    "Besides the method itself, it's definitely worth looking at some of the particulars via which a VedaBase (currently) is returned from `VedaCollection().store`. At the moment, `.store` is the only VedaCollection method which builds and returns a VedaBase out of the data in the cloud. [Here](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/training.py#L490) it is on github, but also look below, I've put it right there, right just barely even down there:\n",
    "\n",
    "\n",
    "```python\n",
    "    def store(self, size, fname, partition=[70, 20, 10], **kwargs):\n",
    "        \"\"\" Build an hdf5 database from this collection and return it as a DataSet instance \"\"\"\n",
    "        namepath, ext = os.path.splitext(fname)\n",
    "        if ext != \".h5\":\n",
    "            fname = namepath + \".h5\"\n",
    "\n",
    "        pgen = self.ids(size)\n",
    "        vb = VedaBase(fname, self.mtype, self.meta['classes'], self.imshape, image_dtype=self.dtype, **kwargs)\n",
    "\n",
    "        build_vedabase(vb, pgen, partition, size, gbdx.gbdx_connection.access_token,\n",
    "                       label_threads=1, image_threads=10)\n",
    "        vb.flush()\n",
    "        return vb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this do? It takes some arguments, instantiates a VedaBase with those args and variety of `self` attributes, calls a function `build_vedabase` that gets passed the VedaBase `vb` as well as some other things. Importantly, it returns the vedabase, which should have snacks inside of it now. \n",
    "\n",
    "The important things to note about this codeblock are the input arguments. I'll go over the main ones here and then include a brief overview of what the build_database function does. \n",
    "\n",
    "\n",
    "* `fname`: a \"local\" filepath which will become your data-filled hdf5 file. I recommend passing in a full system path for the time being.\n",
    "* `size`: the TOTAL number of datapoints you'd like to dump into your vedabase from the cloud. None means all of them (vc.count). \n",
    "* `partition`: What percentage of `size` should be allocated to trainging, testing, and validation, according to you? should change the default to something like 80, 20, 0.\n",
    "\n",
    "This is all you have to provide to `store` in order to get a VedaBase object back. But you should know more. It takes `pgen`, which is a generator of (label, image) urls (`self.ids` is a *generator function*) and passes it to `build_vedabase`, which feeds the generator `pgen` and the vedabase `vb` to a [VedaBaseFetcher](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/fetch/aiohttp/client.py#L253), which, briefly, consumes the generator while fetching the results, running threadpool-based callbacks and writing to the vedabase. This class, and the others in `pyveda.fetch.aiohttp.client` are highly parameterizable in just about every way possible, and hence can be useful tools for concurrent data piping and process/thread pool based processing callbacks across the veda framework. The push/pull patterns utilized there might also serve as dataflow templates that may be useful, or at least informative, during the future development of VedaStream. You can see how it's instantiated and run in [pyveda.fetch.compat.pyfetch3](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/fetch/compat/fetchpy3.py) which is just an awful module name and it should get changed. \n",
    "\n",
    "An aside:\n",
    "*You may notice that it is run in a context manager, `ThreadedAsyncioRunner`, which runs the loop in a real python thread. Why is this necessary? Jupyter notebooks run a jupyter session process that manages communication with the python kernel, and it happens to be asyncio based. In asyncio, each scheduler (loop) owns its own thread, so any other asyncio-based process that's slighty heavy will highjack the session loop, and the kernel will die.*  \n",
    "\n",
    "TODO: notebook documenting how to utilize the various classes in `pyveda.fetch.aiohttp.client.py`, along with brief overview of mechanics. gettttt STOKED!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a basic overview of how and where `VedaCollection` and `VedaBase` interface, along with a brief high-level view of the plumbing that feeds `vb`. This may change in the future; `VedaCollection` and `VedaBase` don't necessarily need to be all that different at the end of the day, but anyway, this access pattern may likely change. Let's get a VedaBase and go through the things about it. There's a bunch of data (vc.count) in this dataset, but I'm late for a cool party probably so I'm going to just grab a 500 count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 3.23 s, total: 15.9 s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fname = '/Users/jamiepolackwich1/projects/veda-nbs/coolparty.h5'\n",
    "vb = vc.store(fname, size=500, title=vc.name, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyveda.db.VedaBase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a VedaBase instance. First thing to do is press tab of course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vb.image_shape:(3, 256, 256)\n",
      "\n",
      "vb.klasses: ['shipping container lot', 'bulldozer', 'truck tractor no trailer', 'facility', 'personal aircraft', 'shed', 'aircraft hangar', 'shipping container', 'passenger vehicle', 'storage tank', 'construction site', 'truck tractor with flatbed trailer', 'helicopter', 'bus', 'fixed wing aircraft', 'cargo truck', 'passenger aircraft', 'truck tractor with liquid tank', 'container crane', 'damaged building', 'building', 'car', 'tractor trailer', 'utility truck', 'vehicle lot', 'trailer no truck', 'truck', 'dump truck', 'utility vehicle', 'helipad']\n",
      "\n",
      "vb.mltype: object_detection\n",
      "\n",
      "vb.train: <pyveda.db.vedabase.WrappedDataNode object at 0x1c2a296748>\n",
      "\n",
      "vb.test: <pyveda.db.vedabase.WrappedDataNode object at 0x1c2a2968d0>\n",
      "\n",
      "vb.validate: <pyveda.db.vedabase.WrappedDataNode object at 0x1c2a296dd8>\n"
     ]
    }
   ],
   "source": [
    "print(\"vb.image_shape:{}\\n\".format(vb.image_shape))\n",
    "print(\"vb.klasses: {}\\n\".format(vb.klasses))\n",
    "print(\"vb.mltype: {}\\n\".format(vb.mltype))\n",
    "print(\"vb.train: {}\\n\".format(vb.train))\n",
    "print(\"vb.test: {}\\n\".format(vb.test))\n",
    "print(\"vb.validate: {}\".format(vb.validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing particularly interesting here, you'll notice that there are discrepancies in semantics between `VedaCollection` and `VedaBase`, eg `imhape` vs. `image_shape`, etc. Those will go away in the future, and those kind of metadata attributes will be consistent across pyveda structures.\n",
    "\n",
    "More interestingly are the `train`, `test` and `validate` properties. These return instances of, what's that you say? `WrappedDataNode` objects. This is a custom class that wraps the concept of a \"node\" or \"group\" in h5 lingo. Nodes are kind of like \"data folders\" which themselves can have attributes and store varieties of data structures, like arrays and tables. `WrappedDataNode` defines useful custom iteration patterns, but first let's press tab again on, say, `vb.train` for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vb.train.images: <pyveda.db.arrays.ImageArray object at 0x1c28d32c88>\n",
      "\n",
      "vb.train.labels: <pyveda.db.arrays.ObjDetectionArray object at 0x1c28d32b38>\n"
     ]
    }
   ],
   "source": [
    "print(\"vb.train.images: {}\\n\".format(vb.train.images))\n",
    "print(\"vb.train.labels: {}\".format(vb.train.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the custom `WrappedDataNode` class, the objects in [pyveda.db.arrays](https://github.com/DigitalGlobe/pyveda/blob/development/pyveda/db/arrays.py) act as hdf5 `array` objects, that is, contiguous, row-accessed chunked data. These data structures are extremely useful in dealing with extraordinarly large amounts of contiguous data, because they provide i/o access *without reading the entire array into memory*. That means we can index and iterate over these objects without worrying about loading everything into memory. Our custom data array classes are smart about things like data serialization and deserialization, batch writes, geo_transforms and more, but also act as low-level wrappers around the core hdf5 array structures.\n",
    "\n",
    "First, let's check that our `build_vedabase` function did the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image, label pairs in vb.train: 350\n",
      "Number of image, label pairs in vb.test: 100\n",
      "Number of image, label pairs in vb.validate: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of image, label pairs in vb.train: {}\".format(len(vb.train)))\n",
    "print(\"Number of image, label pairs in vb.test: {}\".format(len(vb.test)))\n",
    "print(\"Number of image, label pairs in vb.validate: {}\".format(len(vb.validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distribution of data follows the default partition of [70,20,10] that was passed in by default when we called `vc.store()`.\n",
    "\n",
    "We can check the total size as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datapoints in vb: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of datapoints in vb: {}\".format(len(vb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can call them on the individual arrays as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling __len__ on individual array vb.train.labels: 350\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling __len__ on individual array vb.train.labels: {}\".format(len(vb.train.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the array level data access patterns, and then the node level. `__getitem__` and `__iter__` are defined at each level, and the iteration methods are based on generator access provided by the pytables arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array level indexing\n",
    "train_label_0 = vb.train.labels[0]\n",
    "train_label_100 = vb.train.labels[100] \n",
    "train_label_350 = vb.train.labels[-1] \n",
    "\n",
    "\n",
    "train_image_0 = vb.train.images[0]\n",
    "train_image_100 = vb.train.images[100] #vb.train.labels[350]\n",
    "train_image_350 = vb.train.images[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image, label pair 100\n",
      "\n",
      ": [[[ 2559.  2601.  2529. ...,  2165.  2243.  2339.]\n",
      "  [ 2564.  2480.  2518. ...,  2123.  2297.  2450.]\n",
      "  [ 2623.  2604.  2592. ...,  2102.  2223.  2394.]\n",
      "  ..., \n",
      "  [ 2333.  2211.  2320. ...,  2393.  2466.  2389.]\n",
      "  [ 2602.  2376.  2260. ...,  2243.  2426.  2258.]\n",
      "  [ 2191.  2137.  2192. ...,  1970.  2098.  2179.]]\n",
      "\n",
      " [[ 1526.  1554.  1513. ...,  1300.  1352.  1417.]\n",
      "  [ 1530.  1481.  1507. ...,  1276.  1390.  1491.]\n",
      "  [ 1566.  1555.  1552. ...,  1263.  1349.  1466.]\n",
      "  ..., \n",
      "  [ 1376.  1304.  1369. ...,  1507.  1540.  1480.]\n",
      "  [ 1534.  1401.  1336. ...,  1446.  1544.  1419.]\n",
      "  [ 1288.  1258.  1293. ...,  1294.  1356.  1385.]]\n",
      "\n",
      " [[ 1094.  1108.  1079. ...,   927.   963.  1011.]\n",
      "  [ 1091.  1054.  1069. ...,   913.   993.  1064.]\n",
      "  [ 1110.  1103.  1094. ...,   906.   966.  1048.]\n",
      "  ..., \n",
      "  [  975.   923.   966. ...,  1066.  1089.  1043.]\n",
      "  [ 1087.   992.   942. ...,  1026.  1095.  1006.]\n",
      "  [  914.   892.   914. ...,   915.   960.   982.]]]\n",
      "\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(\"training image, label pair 100\\n\\n: {}\\n\\n{}\".format(train_image_100, train_label_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a list of lists indicating bounding boxes (should they be decimal numbers?) and an image to pair. Of course, a `preview` method is on the roster, that would make this demo cooler. But it means that we can readily access individual labels and images on their relevant groups, and they come out they way they should (or, they will if they don't already).\n",
    "\n",
    "Iteration is supported on each array as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is (3, 256, 256), max is 3533.0\n",
      "shape is (3, 256, 256), max is 6113.0\n",
      "shape is (3, 256, 256), max is 8737.0\n",
      "shape is (3, 256, 256), max is 2996.0\n",
      "shape is (3, 256, 256), max is 9484.0\n",
      "shape is (3, 256, 256), max is 9717.0\n",
      "shape is (3, 256, 256), max is 4995.0\n",
      "shape is (3, 256, 256), max is 2790.0\n",
      "shape is (3, 256, 256), max is 8723.0\n",
      "shape is (3, 256, 256), max is 7644.0\n",
      "shape is (3, 256, 256), max is 3675.0\n",
      "shape is (3, 256, 256), max is 2778.0\n",
      "shape is (3, 256, 256), max is 3177.0\n",
      "shape is (3, 256, 256), max is 6458.0\n",
      "shape is (3, 256, 256), max is 3169.0\n",
      "shape is (3, 256, 256), max is 9238.0\n",
      "shape is (3, 256, 256), max is 9688.0\n",
      "shape is (3, 256, 256), max is 8534.0\n",
      "shape is (3, 256, 256), max is 5710.0\n",
      "shape is (3, 256, 256), max is 7696.0\n",
      "shape is (3, 256, 256), max is 5323.0\n",
      "shape is (3, 256, 256), max is 2807.0\n",
      "shape is (3, 256, 256), max is 3084.0\n",
      "shape is (3, 256, 256), max is 5323.0\n",
      "shape is (3, 256, 256), max is 5864.0\n",
      "shape is (3, 256, 256), max is 6841.0\n",
      "shape is (3, 256, 256), max is 9459.0\n",
      "shape is (3, 256, 256), max is 6088.0\n",
      "shape is (3, 256, 256), max is 9159.0\n",
      "shape is (3, 256, 256), max is 3341.0\n",
      "shape is (3, 256, 256), max is 8354.0\n",
      "shape is (3, 256, 256), max is 4930.0\n",
      "shape is (3, 256, 256), max is 3054.0\n",
      "shape is (3, 256, 256), max is 9441.0\n",
      "shape is (3, 256, 256), max is 4324.0\n",
      "shape is (3, 256, 256), max is 9910.0\n",
      "shape is (3, 256, 256), max is 3831.0\n",
      "shape is (3, 256, 256), max is 9669.0\n",
      "shape is (3, 256, 256), max is 7071.0\n",
      "shape is (3, 256, 256), max is 5906.0\n",
      "shape is (3, 256, 256), max is 9539.0\n",
      "shape is (3, 256, 256), max is 8738.0\n",
      "shape is (3, 256, 256), max is 10241.0\n",
      "shape is (3, 256, 256), max is 3178.0\n",
      "shape is (3, 256, 256), max is 3376.0\n",
      "shape is (3, 256, 256), max is 4423.0\n",
      "shape is (3, 256, 256), max is 2617.0\n",
      "shape is (3, 256, 256), max is 5053.0\n",
      "shape is (3, 256, 256), max is 3063.0\n",
      "shape is (3, 256, 256), max is 4296.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for image in vb.validate.images:\n",
    "    print(\"shape is {}, max is {}\".format(image.shape, image.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to call functions on the in-memory images being indexed over without loading them all into memory because of the chunked io access built into hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have builtin access to data *pairs* at the group level, which is especially useful for ML frameworks that consume (label, pair) inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_50, train_image_50 = vb.train[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image, label pair 50\n",
      "\n",
      ": [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [[109.3802144972, 119.5715135001, 148.9118535623, 94.4274131041], [98.7319021398, 99.9617647221, 177.5078828419, 39.7418659506], [207.0744246884, 182.1478976971, 233.4277046956, 143.5353997694], [202.6932695004, 57.1602766449, 233.9508445669, 17.6481480887], [215.5451218351, 212.48630004, 241.898401839, 187.6639701305], [220.7216589854, 249.7198071871, 251.7808818402, 212.9459728887], [240.0160246926, 53.8993049087, 256.0, 15.2868595934], [240.3773783336, 206.050880542, 256.0, 165.5996823646], [0.0, 159.1780360835, 25.0934853957, 80.2230158802]], [[173.7340455397, 70.405823458, 182.4677797424, 54.6907681236], [144.3151513501, 39.6492176251, 161.3229495465, 28.8731829273], [49.8529207959, 109.0199705493, 59.9656656731, 92.4069053279], [17.9058404168, 17.7603983134, 37.2119897285, 3.3923556656000002], [26.6395746111, 138.8785951519, 44.1070430266, 131.694564308], [29.8572661532, 146.5116285263, 45.4860536914, 137.9805912804], [22.9622128343, 126.7555434229, 40.8893514748, 119.1225116607]], [], [], [], [], [[48.2440750169, 188.2688220525, 84.0983523099, 178.3907746095], [62.0341816717, 158.6346828043, 73.0662669904, 129.8985566907], [103.4045016281, 191.8608395662, 142.0168002377, 175.6967618419], [52.8407772378, 207.1269155275, 110.7592251654, 190.0648308024], [122.2509807119, 140.6746029481, 140.6377895725, 128.1025491003], [81.11049586, 177.0437682113, 92.1425811888, 155.9406706141], [90.7635705298, 147.1851315005, 99.9569749568, 122.0410237396], [125.238837142, 147.4096324937, 146.3836673429, 136.6335854614], [45.7158887998, 241.2510942677, 87.5458789722, 222.3929939633], [102.7149962867, 126.9800443649, 113.7470816062, 105.427956203], [93.2917567344, 106.5504606683, 104.323842062, 83.651372131], [117.3095258308, 225.08700806, 173.2743753096, 198.7081232993], [49.8529208035, 131.470063352, 68.6993998848, 122.041023735]], [], [], []]\n",
      "\n",
      "[[[ 1738.  1729.  1726. ...,  2036.  2020.  1970.]\n",
      "  [ 1754.  1806.  1793. ...,  2064.  2024.  2036.]\n",
      "  [ 1742.  1728.  1849. ...,  2126.  2102.  2109.]\n",
      "  ..., \n",
      "  [ 1031.  1183.  1247. ...,  2188.  1914.  1378.]\n",
      "  [ 1133.  1131.  1105. ...,  2136.  1958.  1485.]\n",
      "  [ 1227.  1128.  1111. ...,  2141.  2030.  1596.]]\n",
      "\n",
      " [[ 1222.  1216.  1210. ...,  1349.  1339.  1306.]\n",
      "  [ 1229.  1266.  1255. ...,  1354.  1331.  1336.]\n",
      "  [ 1217.  1208.  1293. ...,  1385.  1372.  1376.]\n",
      "  ..., \n",
      "  [  791.   918.   981. ...,  1372.  1200.   865.]\n",
      "  [  869.   881.   870. ...,  1340.  1228.   934.]\n",
      "  [  948.   883.   876. ...,  1343.  1273.  1001.]]\n",
      "\n",
      " [[  987.   985.   982. ...,  1001.   992.   970.]\n",
      "  [  982.  1016.  1008. ...,  1003.   986.   992.]\n",
      "  [  959.   957.  1025. ...,  1026.  1014.  1019.]\n",
      "  ..., \n",
      "  [  551.   632.   666. ...,  1007.   884.   639.]\n",
      "  [  604.   605.   590. ...,   987.   905.   691.]\n",
      "  [  653.   605.   594. ...,   993.   943.   744.]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"training image, label pair 50\\n\\n: {}\\n\\n{}\".format(train_image_50, train_label_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get slices as well (same for arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first10_train_image_label_pairs = vb.train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(first10_train_image_label_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "img0, lbl0 = first10_train_image_label_pairs[0]\n",
    "print(np.array_equiv(img0, train_image_0))\n",
    "print(lbl0 == train_label_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also iterate over image, label pairs on nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im_lbl_pair in vb.train:\n",
    "    pass\n",
    "    # im_lbl_pair is [image, label] tuple\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much the gist of data access patterns in `VedaBase`. On that (data-access) front, coming soon are:\n",
    "* ordering options for node access (eg, (label, image) instead of (image, label))\n",
    "* batch iteration (eg, for dps in vb.test[0:100:10])\n",
    "\n",
    "One word of **caution** when indexing on arrays: as soon as you call `__getitem__` with a `slice` object, eg `100images = db.train.images[100:200]`, you have loaded those data into **memory**. Therefore, for very large arrays, `images = db.train.images[:]` could very well overload your process ram and cause problems, just like any large im-memory data.\n",
    "\n",
    "When you're done using your vedabase, it's important that you close the file handle. If you have added or changed data at all, you should also call flush(). The proper cleanup is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb.flush()\n",
    "vb.close()\n",
    "del vb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reload your vedabase at another time, instantiate a VedaBase object with the path to your file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyveda.db import VedaBase\n",
    "vb = VedaBase(fname, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image, label pairs in vb.train: 350\n",
      "Number of image, label pairs in vb.test: 100\n",
      "Number of image, label pairs in vb.validate: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of image, label pairs in vb.train: {}\".format(len(vb.train)))\n",
    "print(\"Number of image, label pairs in vb.test: {}\".format(len(vb.test)))\n",
    "print(\"Number of image, label pairs in vb.validate: {}\".format(len(vb.validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the expected results.\n",
    "\n",
    "`VedaBase` also supports inputting custom loader functions, which can be any transform you please, but can be especially useful for implementing the specific structures/formats that various ML frameworks require on consumption or training. Future work will supply a module of plugins for well-known frameworks like TensorFlow, PyTorch etc, so that they can be loaded behind the scenes with a keyword argument (or something like that). That's it for now, check back in next big push to development, and hit me up (@rjpolackwich on github) with questions, and don't hesistate to submit an issue (and make a PR for it?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb.close()\n",
    "del vb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
