{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyveda PR-55: An Overview\n",
    "\n",
    "This is an overview to the changes introduced in pyveda PR #55, \"Flat arrays pave the way.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background/motivation\n",
    "\n",
    "The original motivation for this work is rooted in the general frustration that grew out the manner in which the pyveda api provides mutliple data access modes - each specifically designed object structures meant to address some particular ML-problem-based workflow or use-case (or a part thereof) - as first-class objects. The source of this frustration was principally singular, but two-fold manifest: both at the user level and developer level. \n",
    "\n",
    "At the public level, all but the most trivial use cases require a user to interface with multiple data access objects, each parameterized by seemingly arbitrarily self-similar/intersecting input parameters along with their own specific input arguments (often technical in nature), without a paved path in between. Besides the technical onus this places on any user with even a shell of a real-world problem working in the veda system/api, this design enforces a conceptual construct that has the effect of ascribing some kind of fundamentality to those objects in the framework, which implies an inherent and unique purpose. What that means is that, eg, any user is required to conceptualize their workflow in terms of, in general, an interface to Veda, a network-based io object, and a disk-based io object, all of which provide similar methods (eg, iteration interfaces) and require similar data parameters (potentially), but without much of any straighforward mapping interfaces between them. \n",
    "\n",
    "Internal development is similarly affected by this emergent component-based framework conceptualization. In the virtuous pursuit of providing consistent, self-similar extension functions or classes, developers are inevitably tasked with engineering multiple versions of components due to the multiplicity of the base access structures, frequently requiring the manual bushwacking that goes along with passing one structure to the initialization/parameterization of another, all under the table. Not only does this make progress inefficient as well as introduces lots of transitory parameterization code, but it introduces hard requirements between the underlying objects themselves, often static, which can be fault-prone to core development as well as potentially constrain further development of objects that extend or incorporate them in any way. All the while, we perpetuate the conceptual framework of these apparently unique components throughout our codebase as atomic, self-contained freakin things.\n",
    "\n",
    "Why is it like this? The basic reason is because early development was done in parallel and sought to provide the basic methods required to interface with Veda as well as the technically based io-optimized components, meant to serve data from veda, but designed for agnostic data at the same time. Parameterization intoduced nuances in such a way that made it difficult to both provide general and flexible instantiation protocols. Moreover, meaningful parameter scopes were changing often, with new ones being introduced as necessary as we learned from our engineering as well as the enormous problem scope, which still represents one of the biggest challenges facing our engineering team as the project matures and use cases discovered.\n",
    "\n",
    "\n",
    "## Concepts/approach\n",
    "The first step in mitigating the current framework is to build wide-open paths between the accessor objects that already exist, and provide the methods that are the building blocks of our api. One way to do that is to programmatically and conceptually separate the data parameters that characterize our datasets and the fundamental operations and apis that the accessors provide, while still providing protocols to incorporate the nuances those data parameters introduce on the objects themselves. One way to try to do that is to try to make a very general object from which everything else can derive from while providing flexibility for specification down the line. \n",
    "\n",
    "`BaseDataSet` provides an interface that any obeject that provides the usual `train`, `test`, `validate` interface and the methods needed to be specified for that. `BaseSampleArray` provides an analogous protocol, and is very much a close proxy to `BaseDataSet`, inhereting presecribd methods on look-up from it's parent dataset. Notice that `BaseDataSet` defines no data parameter properties on its code body; instead, these properties are defined as class properties and are populated from instantiation arguments according to the `_vprops` specification. The `_vprops` specification is a dictionary of relevant property names and values, and are all functions that specify the _descriptor_ protocol. Subclasses can inheret these properties, define their own, add new, or exclude using the `register_vprops` wrapper. One consequence of this is that we can delegate all data parameter initialization in any child class down to `BaseDataSet`, making the initialization of our accessor objects specific to the object itself, making them more flexible to use and more clear what they do. Take a look, for instance, at the initialization signature for `H5DataBase`:\n",
    "\n",
    "    class H5DataBase(BaseDataSet):\n",
    "        \"\"\"\n",
    "        An interface for consuming and reading local data intended to be used with\n",
    "        machine learning training\n",
    "        \"\"\"\n",
    "        _sample_class = H5SampleArray\n",
    "        _variable_class = H5VariableArray\n",
    "\n",
    "        def __init__(self, fname, title=\"SBWM\", overwrite=False, mode=\"a\", **kwargs):\n",
    "        \n",
    "Our object requires a filename, specifically, with some object-specific keyword arguments, and passes `**kwargs` down to `BaseDataSet`, which handles them appropriately. We don't even need to spec them to get a basic H5 object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyveda.vedaset import VedaBase\n",
    "import os\n",
    "\n",
    "fname = os.getcwd() + \"/temp.h5\"\n",
    "vb = VedaBase(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, most interface methods that depend on data parameters will fail at the moment, but we could use this object to, eg, simply write out dataset ids. We can also assign data parameters dynamically, so that our `VedaBase` can be naively passed to another object and specified or customized as needed at a later time, or maybe we want to build a database from non-veda data. `VedaStream` can be similarly naievly instantiated.\n",
    "\n",
    "\n",
    "### Data descriptors and local actions (flat arrays: paving ways)\n",
    "\n",
    "Some of the properties we commonly pass around to describe datasets shouldn't necessarily need to be static in nature. For instance, we may want to specify additional classes on our dataset when concatenating outside data, or remove classes based on some representative metric. But what if that property is used by the data accessor in one or more op critical ways? The accessor object needs to be able to take the appropriate actions as necessary. \n",
    "\n",
    "Suppose we have some spatio-temporal vedabase dataset where N is small, but new data becomes available each day. In general, model accuracy scales... well exponentially in the low N regime. So after getting crappy results with a [70, 30, 20] data partition, we decide to load the training set and test our results on new data that will come in in the future. What happens when we write `vb.partition = [90, 0, 10]`?\n",
    "\n",
    "Currently, nothing. Data is physically partitioned according to the `.partition` distribution set during the write process to three different arrays. That might be some default value like 70, 30, 10 that we didn't even specify. There is no support for arbitrary iteration over virtually grouped arrays of arbitrary length, that would be ridiculous. So literally, nothing changes: your size of your training data is the same as it was before, there's not even a warning (which is messed up). Dynamic group partitioning is a simple but critical kind of feature that is central to the value-prop domain of pyveda. To support this, `H5DataBase` structures have been re-engineered to write data to single, flat arrays, and some new array interfaces have been introduced to handle the delicacies of calculating and maintaining virtual indexes instead. This turned out to have immediate potential engineering gains across the codebase, for instance:\n",
    "* partition-based h5 batch-writes off of the client stream became way simpler\n",
    "* as a consequence, this opened the door to consolidate VedaStream and VedaBase io clients into a single boss client\n",
    "* Stream, Base array wrapper classes effectively identical, consolidating iteration interfaces and opening door for accessor-agnostic extension classes and plugins for any implementer of virtual-indexed based access-pattern\n",
    "* Less complicated file-struture; H5DataBase core functionality effectively taken care of by BaseDataSet, bridging functionality structure gap between stream and base\n",
    "\n",
    "Those are exciting prospects, especially in the context of the approach outlined above. However, the `partition` data descriptor needs some way to schedule a call back to the object so that the relevant actions are taken, according to the object it lives on, which is a nontrivial design challenge.\n",
    "\n",
    "\n",
    "### Descriptor callbacks\n",
    "`BaseDataSet` has a private attribute, `._prc`. This is a special attribute, and can be used to _register custom callbacks_ on properties, depending on how a subclass down the line might need to respond when a property is assigned with a certain value, or changes in a certain way. This works due to a special descriptor mixin that looks and checks if `._prc` exists on object calling it, and if it does, it attempts to make any callbacks registered to its name. The base descriptor object is pretty straightforward and implements the classic decriptor pattern:\n",
    "\n",
    "    class BaseDescriptor(object):\n",
    "        __vname__ = NotImplemented\n",
    "\n",
    "        def __init__(self, **kwargs):\n",
    "            self.__dict__.update(kwargs)\n",
    "            if not getattr(self, \"name\", None):\n",
    "                self.name = type(self).__vname__\n",
    "\n",
    "        def __get__(self, instance, klass):\n",
    "            if instance is None:\n",
    "                return self\n",
    "            try:\n",
    "                return instance.__dict__[self.name]\n",
    "            except KeyError as ke:\n",
    "                raise AttributeError(\"'{}' object has no attribute '{}'\"\n",
    "                                     .format(type(instance).__name__, self.name)) from None\n",
    "\n",
    "        def __set__(self, instance, value):\n",
    "            instance.__dict__[self.name] = value\n",
    "            \n",
    "         \n",
    "As a python property, this looks something like:\n",
    "\n",
    "    @property\n",
    "    def namedattr(self):\n",
    "        return self.name\n",
    "        \n",
    "    @namedattr.setter\n",
    "    def namedattr(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "Since our descriptors are meant to be object-agnostic for general applicability, we can define a mixin that, instead of holding some callback state locally, looks to the object it's describing and checks for the special `._prc` attribute, which holds a catalog of callbacks indexed by property name:\n",
    "\n",
    "    class PropCallbackExecutor(BaseDescriptor):\n",
    "    registry_target = \"_prc\"\n",
    "\n",
    "    def __set__(self, instance, value):\n",
    "        super().__set__(instance, value)\n",
    "        registry = getattr(instance, self.registry_target, None)\n",
    "        if registry:\n",
    "            for cb in registry[self.name]:\n",
    "                if inspect.ismethod(cb):\n",
    "                    cb(self.name, value)\n",
    "                else:\n",
    "                    cb(self.name, value, instance)\n",
    "                    \n",
    "If the object has a property callback register accessible via `._prc`, this descriptor mixin first sets the value on the object and then proceeds to execute any callbacks the object has registered on its name. Depending on the type of callback (function, method), the callback should support the relevant call signature. Delegating the callback state to the object gives the object dynamic callback control, and thus is exposed at the interface level programmatically.\n",
    "\n",
    "\n",
    "This turns out to be a useful construct; different objects may require various property callbacks. For VedaStream and VedaBase, updating either `count` or `partition` should change group allocation, and in the context of VedaBase, update the virtual index cache `._vidx`. Let's see how this works with example objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "70 20 10\n"
     ]
    }
   ],
   "source": [
    "import pyveda as pv\n",
    "pv.config.set_dev()\n",
    "        \n",
    "from pyveda.vedaset import VedaStream, VedaBase\n",
    "vc = pv.from_id(\"94493508-dd19-4d30-b207-2466ecfc0d2f\")\n",
    "\n",
    "source = vc.gen_sample_ids(count=100)\n",
    "vs = VedaStream(source, write_h5=False, write_index=False,\n",
    "        mltype=vc.mltype, classes=vc.classes, image_shape=vc.imshape, image_dtype=vc.dtype,\n",
    "        count=100, partition=[70,20,10])\n",
    "\n",
    "print(vs.train.allocated, vs.test.allocated, vs.validate.allocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group count allocations as expected. Changing the partition or the count should change these numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Probability distribution must sum to 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0fe0d6fbe205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pyveda/pyveda/vedaset/props.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# size desc? lol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size '{}' must match '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyveda/pyveda/vedaset/props.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability distribution must sum to 100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Probability distribution must sum to 100"
     ]
    }
   ],
   "source": [
    "vs.partition = [90, 10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize our data descriptors as well - setting expected types, checksums, size checks, etc. Check out the `pyveda.vedaset.props` module for all available type descriptions and how they can be subclassed and mixed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 5 5\n"
     ]
    }
   ],
   "source": [
    "vs.partition = [90, 5, 5]\n",
    "print(vs.train.allocated, vs.test.allocated, vs.validate.allocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vedabase requires actual index values, which must update on `count` or `partition` assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vb = VedaBase(fname, overwrite=True, **vs._unpack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 0 40\n",
      "[0, 159] [160, 159] [160, 199]\n"
     ]
    }
   ],
   "source": [
    "vb.partition = [80, 0, 20]\n",
    "vb.count = 200\n",
    "print(vb.train.allocated, vb.test.allocated, vb.validate.allocated)\n",
    "print([vb.train.images._start, vb.train.images._stop], [vb.test.images._start, vb.test.images._stop],[vb.validate.images._start, vb.validate.images._stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering descriptor callbacks is accomplished via the CatalogRegister api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_callback(name, value):\n",
    "    print(\"Hello from classes descriptor!\")\n",
    "    print(value)\n",
    "    \n",
    "vb._prc.classes.register(classes_callback)\n",
    "classes = vb.classes\n",
    "classes.append(\"Horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from classes descriptor!\n",
      "['boat', 'Horse']\n"
     ]
    }
   ],
   "source": [
    "vb.classes = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, property callbacks should be registered before passing initialization `**kwargs` to `BaseDataSet` so that they are ready for action when the properties are set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
